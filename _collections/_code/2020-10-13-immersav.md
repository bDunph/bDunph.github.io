---
layout: post
title: ImmersAV Toolkit  
author: Bryan Dunphy 
date: 13/10/2020
image: ../assets/images/obj3EnvDemo10.png 
---

[ImmersAV](https://github.com/bDunph/ImmersAV) is an open source toolkit for immersive audiovisual composition. It was built around a focused approach to composition based on generative audio, raymarching and interactive machine learning techniques. Here are two examples of work created with the toolkit:

[obj\_#3](https://www.youtube.com/watch?v=RdvezMCTt-I&feature=youtu.be)

[Ag Fás Ar Ais Arís](https://www.youtube.com/watch?v=LewHeC5e1fM&t=5s)

Aims:
- Provide well defined, independent areas for generating audio and visual material.
- Provide a class that can be used to generate, send and receive data from:
	- machine learning algorithms
	- VR hardware sensors
	- audio and visual processing engines
- Allow for direct rendering on a VR headset.

Dependencies:
- OpenVR
- Csound6
- OpenGL4
- glm
- glfw3
- Glew
- CMake3
- RapidLib
- libsndfile

See the [GitHub repo](https://github.com/bDunph/ImmersAV) for detailed instructions for installation, workflow and walkthroughs.
